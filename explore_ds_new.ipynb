{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from sparc.client import SparcClient\n",
    "\n",
    "# === Setup ===\n",
    "client = SparcClient(connect=False, config_file='config.ini')\n",
    "\n",
    "# === Step 1: Run Elasticsearch-style query to get a known ID range ===\n",
    "ids = list(range(0, 1001))\n",
    "id_strings = [f'\"{i}\"' for i in ids]\n",
    "id_list_str = \", \".join(id_strings)\n",
    "\n",
    "body = f'''\n",
    "{{\n",
    "  \"size\": 1000,\n",
    "  \"query\": {{\n",
    "    \"terms\": {{\n",
    "      \"_id\": [ {id_list_str} ]\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "'''\n",
    "body_json = json.loads(body)\n",
    "response = client.metadata.search_datasets(body_json)\n",
    "\n",
    "# === Step 2: Filter and categorize by type ===\n",
    "type_counter = Counter()\n",
    "type_id_map = defaultdict(list)\n",
    "\n",
    "for d in response[\"hits\"][\"hits\"]:\n",
    "    try:\n",
    "        type_name = d[\"_source\"][\"item\"][\"types\"][0][\"name\"]\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        type_name = \"<invalid or missing type>\"\n",
    "\n",
    "    type_counter[type_name] += 1\n",
    "    dataset_id = d.get(\"_id\", \"<no id>\")\n",
    "    type_id_map[type_name].append(dataset_id)\n",
    "\n",
    "# === Step 3: Show summary ===\n",
    "print(\"Dataset type counts:\")\n",
    "for t, count in type_counter.items():\n",
    "    print(f\"{t}: {count}\")\n",
    "\n",
    "print(\"\\nProcessing only type = 'dataset'\")\n",
    "dataset_ids = type_id_map[\"dataset\"]\n",
    "print(f\"Found {len(dataset_ids)} datasets.\\n\")\n",
    "\n",
    "# === Step 4: Process only datasets and collect extensions ===\n",
    "dataset_paths = []\n",
    "non_dataset_count = 0\n",
    "error_count = 0\n",
    "extension_counter = Counter()\n",
    "\n",
    "for dataset_id in tqdm(dataset_ids, desc=\"Processing datasets\"):\n",
    "    # === Fetch metadata ===\n",
    "    metadata_url = f\"https://api.pennsieve.io/discover/datasets/{dataset_id}/versions/1/metadata\"\n",
    "    try:\n",
    "        meta_response = requests.get(metadata_url)\n",
    "        meta_response.raise_for_status()\n",
    "        metadata = meta_response.json()\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"[ERROR] Metadata fetch failed for dataset {dataset_id}: {e}\")\n",
    "        error_count += 1\n",
    "        continue\n",
    "\n",
    "    # === Extract 'files/primary/' paths and extensions ===\n",
    "    primary_paths = [\n",
    "        f.get(\"path\") for f in metadata.get(\"files\", [])\n",
    "        if f.get(\"path\", \"\").startswith(\"files/primary/\")\n",
    "    ]\n",
    "\n",
    "    if primary_paths:\n",
    "        dataset_paths.append((dataset_id, primary_paths))\n",
    "        for path in primary_paths:\n",
    "            _, ext = os.path.splitext(path)\n",
    "            ext = ext.lower() if ext else \"<no extension>\"\n",
    "            extension_counter[ext] += 1\n",
    "\n",
    "        tqdm.write(f\"  Primary file paths for dataset {dataset_id}: {primary_paths}\")\n",
    "    else:\n",
    "        tqdm.write(f\"  No files under 'files/primary/' for dataset {dataset_id}.\")\n",
    "        non_dataset_count += 1\n",
    "\n",
    "# === Final Summary ===\n",
    "tqdm.write(\"\\n=== Summary ===\")\n",
    "tqdm.write(f\"Total datasets queried: {len(dataset_ids)}\")\n",
    "tqdm.write(f\"Metadata errors: {error_count}\")\n",
    "tqdm.write(f\"Datasets without primary files: {non_dataset_count}\")\n",
    "tqdm.write(f\"Datasets with primary files: {len(dataset_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extension Summary ===\n",
    "tqdm.write(\"\\nAvailable file extensions in 'files/primary/':\")\n",
    "for ext, count in sorted(extension_counter.items(), key=lambda x: -x[1]):\n",
    "    if count >= 100:  # Only show extensions with significant counts\n",
    "        tqdm.write(f\"{ext}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "# Use seaborn for cleaner minimal plots\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = './stats_figures'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Example input structure ===\n",
    "# dataset_paths = [('ds1', ['file1.mat', 'image1.tif']), ('ds2', ['notes.pdf', 'data.csv'])]\n",
    "\n",
    "# Map extensions to categories, merging Tabular and Structured into Docs\n",
    "modality_lookup = {\n",
    "    # Imaging formats\n",
    "    \".tif\": \"Imaging\", \".tiff\": \"Imaging\", \".czi\": \"Imaging\", \".nd2\": \"Imaging\", \".lsm\": \"Imaging\",\n",
    "    \".jpx\": \"Imaging\", \".svs\": \"Imaging\", \".ims\": \"Imaging\", \".png\": \"Imaging\", \".jpg\": \"Imaging\",\n",
    "    \".jpeg\": \"Imaging\", \".bmp\": \"Imaging\", \".vsi\": \"Imaging\", \".jp2\": \"Imaging\", \".roi\": \"Imaging\",\n",
    "    \".dm3\": \"Imaging\", \".pxp\": \"Imaging\", \".ipf\": \"Imaging\", \".lif\": \"Imaging\", \".ima\": \"Imaging\",\n",
    "    \".mrxs\": \"Imaging\", \".obj\": \"Imaging\", \".avi\": \"Imaging\", \".exf\": \"Imaging\", \".cxd\": \"Imaging\",\n",
    "\n",
    "    # Time Series formats\n",
    "    \".mat\": \"Time Series\", \".smr\": \"Time Series\", \".csv\": \"Time Series\",\n",
    "    \".adicht\": \"Time Series\", \".hdf5\": \"Time Series\", \".h5\": \"Time Series\", \".ets\": \"Time Series\",\n",
    "    \".abf\": \"Time Series\", \".rhd\": \"Time Series\", \".nev\": \"Time Series\", \".ns5\": \"Time Series\",\n",
    "    \".ns2\": \"Time Series\", \".ns1\": \"Time Series\", \".smrx\": \"Time Series\", \".wav\": \"Time Series\",\n",
    "    \".acq\": \"Time Series\", \".tbk\": \"Time Series\", \".tdx\": \"Time Series\", \".tev\": \"Time Series\",\n",
    "    \".tin\": \"Time Series\", \".tnt\": \"Time Series\", \".tsq\": \"Time Series\", \".eeg\": \"Time Series\",\n",
    "    \".vmrk\": \"Time Series\", \".vhdr\": \"Time Series\", \".sev\": \"Time Series\", \".sam\": \"Time Series\",\n",
    "    \".pss\": \"Time Series\", \".psmethod\": \"Time Series\",\n",
    "\n",
    "    # Documentation formats\n",
    "    \".pdf\": \"Docs\", \".docx\": \"Docs\", \".doc\": \"Docs\", \".txt\": \"Docs\",\n",
    "    \".xlsx\": \"Docs\", \".xls\": \"Docs\", \".tsv\": \"Docs\", \".json\": \"Docs\", \n",
    "    \".xml\": \"Docs\", \".db\": \"Docs\", \".xfg\": \"Docs\",\n",
    "    \n",
    "    # Other formats\n",
    "    \".inf\": \"Other\", \".zip\": \"Other\", \"\": \"Other\", \"(no ext)\": \"Other\",\n",
    "    \".s2r\": \"Other\", \".ini\": \"Other\", \".cmgui\": \"Other\",\n",
    "    \".mp4\": \"Other\", \".gz\": \"Other\", \".xlsm\": \"Other\",\n",
    "    \".db3\": \"Other\", \".ccf\": \"Other\", \".ex\": \"Other\",\n",
    "    \".conf\": \"Other\", \".rdf\": \"Other\", \".vtk\": \"Other\", \".proj\": \"Other\", \".pnp\": \"Other\",\n",
    "    \".hoc\": \"Other\", \".fig\t\": \"Other\", \".dat\": \"Other\"\n",
    "}\n",
    "\n",
    "# === Summary structures ===\n",
    "summary = {}\n",
    "modality_extension_counts = defaultdict(Counter)\n",
    "dataset_category = defaultdict(set)\n",
    "datasets_with_timeseries = set()\n",
    "\n",
    "# === Process the dataset file paths ===\n",
    "for dataset_id, paths in dataset_paths:\n",
    "    has_ts = False\n",
    "    has_img = False\n",
    "    has_docs = False\n",
    "    for p in paths:\n",
    "        ext = os.path.splitext(p)[1].lower() or '(no ext)'\n",
    "        category = modality_lookup.get(ext, 'Other')\n",
    "        summary[ext] = summary.get(ext, 0) + 1\n",
    "        modality_extension_counts[category][ext] += 1\n",
    "        if category == 'Time Series':\n",
    "            has_ts = True\n",
    "        elif category == 'Imaging':\n",
    "            has_img = True\n",
    "        elif category == 'Documentation':\n",
    "            has_docs = True\n",
    "        dataset_category[category].add(dataset_id)\n",
    "    if has_ts:\n",
    "        datasets_with_timeseries.add(dataset_id)\n",
    "    elif has_img:\n",
    "        dataset_category['Imaging'].add(dataset_id)\n",
    "    elif has_docs:\n",
    "        dataset_category['Documentation'].add(dataset_id)\n",
    "    else:\n",
    "        dataset_category['Other'].add(dataset_id)\n",
    "\n",
    "\n",
    "# === Create summaries ===\n",
    "summary_df = pd.DataFrame.from_dict(summary, orient='index', columns=['File Count'])\n",
    "summary_df.index.name = 'Extension'\n",
    "summary_df['Category'] = summary_df.index.map(lambda x: modality_lookup.get(x, 'Other'))\n",
    "summary_df = summary_df.sort_values('File Count', ascending=False)\n",
    "\n",
    "category_summary = (\n",
    "    summary_df[summary_df['Category'] != 'Unknown']\n",
    "    .groupby('Category')['File Count']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "dataset_series = pd.Series({cat: len(ids) for cat, ids in dataset_category.items() if cat != 'Unknown'}).sort_values(ascending=False)\n",
    "ts_series = pd.Series(modality_extension_counts['Time Series']).sort_values(ascending=False)\n",
    "img_series = pd.Series(modality_extension_counts['Imaging']).sort_values(ascending=False)\n",
    "\n",
    "# === Plot configuration ===\n",
    "TICK_FS = 10\n",
    "LABEL_FS = 12\n",
    "\n",
    "# For a half 16:9 slide (6 x 8 inches)\n",
    "fig, axes = plt.subplots(4, 1, figsize=(6, 8))\n",
    "\n",
    "# Y-axis label texts for each subplot\n",
    "y_labels = ['Files', 'Datasets', 'Time Series Files', 'Imaging Files']\n",
    "i = 0\n",
    "for ax, data, ylab in zip(axes, [category_summary, dataset_series, ts_series, img_series], y_labels):\n",
    "    if i<=1:\n",
    "        #filter out docs\n",
    "        data = data[data.index != 'Docs']\n",
    "    ax.bar(data.index, data.values, color=sns.color_palette('plasma', len(data)))\n",
    "    ax.tick_params(axis='x', labelsize=TICK_FS)\n",
    "    ax.tick_params(axis='y', labelsize=TICK_FS)\n",
    "    ax.set_ylabel(ylab, fontsize=LABEL_FS)\n",
    "    # Scientific notation\n",
    "    if i==0:\n",
    "        ax.ticklabel_format(style='scientific', axis='y', scilimits=(0,0), useMathText=True)\n",
    "    # Match offset text size\n",
    "    offset = ax.yaxis.get_offset_text()\n",
    "    offset.set_fontsize(TICK_FS)\n",
    "    i += 1\n",
    "\n",
    "# Rotate x-ticks for last two\n",
    "for ax in axes[2:]:\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'combined_4x1_summary_minimal_sorted.svg'), format='svg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# === Word Cloud: File Extension Frequencies ===\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Filter extension_counter to only include counts â‰¥ 100\n",
    "wc_freq = {\n",
    "    ext.lstrip('.') or 'no_ext': count\n",
    "    for ext, count in extension_counter.items()\n",
    "    if count >= 1\n",
    "}\n",
    "\n",
    "# Generate word cloud\n",
    "graph = WordCloud(width=1600, height=800, background_color='white').generate_from_frequencies(wc_freq)\n",
    "\n",
    "# Plot word cloud\n",
    "plt.figure(figsize=(5, 2.5))\n",
    "plt.imshow(graph)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "# Save the word cloud\n",
    "plt.savefig(os.path.join(output_dir, 'file_extensions_wordcloud.svg'), format='svg')\n",
    "plt.show()\n",
    "\n",
    "# print ids per category\n",
    "print(\"\\nDataset IDs per category:\")\n",
    "print(\"=== Imaging Datasets ===\")\n",
    "print(f\"({len(dataset_category['Imaging'])}): {', '.join(map(str, dataset_category['Imaging']))}\")\n",
    "print(\"\\n=== Time Series Datasets ===\")\n",
    "print(f\"({len(dataset_category['Time Series'])}): {', '.join(map(str, dataset_category['Time Series']))}\")\n",
    "print(\"\\n=== Documentation Datasets ===\")\n",
    "print(f\"({len(dataset_category['Documentation'])}): {', '.join(map(str, dataset_category['Documentation']))}\")\n",
    "print(\"\\n=== Other Datasets ===\")\n",
    "print(f\"({len(dataset_category['Other'])}): {', '.join(map(str, dataset_category['Other']))}\")\n",
    "\n",
    "# print totla number of file formats\n",
    "print(f\"\\nTotal number of unique file formats: {len(modality_lookup)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparc_vns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
