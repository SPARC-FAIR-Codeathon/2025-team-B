{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbf5984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/miniconda3/envs/sparc_vns/lib/python3.10/site-packages/sparc/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary files:  ['files/primary/sub-ChAT-Male-Subject-1/20_1021.acq', 'files/primary/sub-ChAT-Male-Subject-2/20_1022.acq', 'files/primary/sub-ChAT-Female-Subject-1/20_1023.acq', 'files/primary/sub-ChAT-Male-Subject-3/20_1027a.acq', 'files/primary/sub-ChAT-Female-Subject-2/20_1027b.acq', 'files/primary/sub-ChAT-Female-Subject-3/20_1028.acq', 'files/primary/sub-nNOS-Male-Subject-1/20_1202a.acq', 'files/primary/sub-nNOS-Male-Subject-2/20_1202b.acq', 'files/primary/sub-nNOS-Male-Subject-3/20_1203a.acq', 'files/primary/sub-nNOS-Female-Subject-1/20_1203b.acq', 'files/primary/sub-nNOS-Female-Subject-2/20_1207.acq', 'files/primary/sub-nNOS-Female-Subject-3/20_1208.acq', 'files/primary/manifest.xlsx']\n",
      "[WARN] Failed to load descriptor from ./mapping_schemes/mapping_scheme_400_adi.py: No module named 'adi._adi_cffi'\n",
      "[WARN] Failed to load descriptor from ./mapping_schemes/mapping_scheme_378.py: No module named 'adi._adi_cffi'\n",
      "[INFO] Loaded 32 descriptor(s) from ./mapping_schemes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset 224: 100%|██████████| 1/1 [00:14<00:00, 14.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'rel_path': 'primary/sub-ChAT-Male-Subject-1/20_1021.acq',\n",
       "  'local_path': '/var/folders/34/5y88z7rd5ml0ycwmy2g3ntrw0000gn/T/tmpe7z784vt/20_1021.acq',\n",
       "  'std_path': 'output_single/20_1021_std.zarr',\n",
       "  'descriptor_id': 'acq_mapping_001',\n",
       "  'mapping_score': 3,\n",
       "  'status': 'ok',\n",
       "  'error': None}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "from sparc_fuse_core import (list_primary_files,\n",
    "                      download_and_convert_sparc_data)\n",
    "\n",
    "DATASET_ID = 224\n",
    "\n",
    "files, meta = list_primary_files(DATASET_ID)\n",
    "\n",
    "print(\"primary files: \", [f[\"path\"] for f in files])\n",
    "\n",
    "# Case 1 – convert a single file\n",
    "download_and_convert_sparc_data(\n",
    "    DATASET_ID,\n",
    "    primary_paths=files[0][\"path\"].replace(\"files/\", \"\"),\n",
    "    output_dir=\"./output_single\", \n",
    "    file_format=\"zarr\"  # or \"zarr.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b3e4a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated metadata.\n"
     ]
    }
   ],
   "source": [
    "import zarr, s3fs\n",
    "\n",
    "# Adjust region if needed (your bucket is in eu-north-1)\n",
    "fs = s3fs.S3FileSystem(anon=False, client_kwargs={\"region_name\": \"eu-north-1\"})\n",
    "s3_store = zarr.storage.FSStore(\"s3://sparc-fuse-demo-ab-2025/20_1021_std.zarr\", fs=fs)\n",
    "\n",
    "# Consolidate metadata for faster remote traversal\n",
    "zarr.consolidate_metadata(s3_store)\n",
    "print(\"Consolidated metadata.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4793ed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zarr tree:\n",
      "signals shape: (4, 1408754) attrs: <zarr.attrs.Attributes object at 0x31cf0d660>\n",
      "time shape: (1408754,) attrs: <zarr.attrs.Attributes object at 0x31cf0ca90>\n",
      "\n",
      "Constructed xarray Dataset:\n",
      "<xarray.Dataset> Size: 56MB\n",
      "Dimensions:  (channel: 4, time: 1408754)\n",
      "Coordinates:\n",
      "  * time     (time) float64 11MB 0.0 0.01 0.02 ... 1.409e+04 1.409e+04 1.409e+04\n",
      "  * channel  (channel) int64 32B 0 1 2 3\n",
      "Data variables:\n",
      "    signals  (channel, time) float64 45MB -0.2869 -0.3036 ... 0.0008336\n",
      "\n",
      "Example slice: <xarray.DataArray 'signals' (channel: 4)> Size: 32B\n",
      "array([-0.28689462, -0.33375651,  0.02725497,  0.00112855])\n",
      "Coordinates:\n",
      "    time     float64 8B 0.0\n",
      "  * channel  (channel) int64 32B 0 1 2 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"eu-north-1\"\n",
    "\n",
    "import s3fs\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Connect to S3 Zarr\n",
    "fs = s3fs.S3FileSystem(anon=False, client_kwargs={\"region_name\": \"eu-north-1\"})\n",
    "store = zarr.storage.FSStore(\"s3://sparc-fuse-demo-ab-2025/20_1021_std.zarr\", fs=fs)\n",
    "\n",
    "# Open (prefer consolidated metadata if available)\n",
    "try:\n",
    "    root = zarr.open_consolidated(store)\n",
    "except Exception:\n",
    "    root = zarr.open(store)\n",
    "\n",
    "# Show structure\n",
    "print(\"Zarr tree:\")\n",
    "root.tree()\n",
    "\n",
    "# Inspect arrays\n",
    "signals_arr = root[\"signals\"]\n",
    "time_arr = root[\"time\"]\n",
    "print(\"signals shape:\", signals_arr.shape, \"attrs:\", signals_arr.attrs)\n",
    "print(\"time shape:\", time_arr.shape, \"attrs:\", time_arr.attrs)\n",
    "\n",
    "# Load arrays (lazy until sliced)\n",
    "signals = signals_arr[:]  # full read; can slice instead if large\n",
    "time = time_arr[:]\n",
    "\n",
    "# Heuristic: align dimensions\n",
    "if signals.shape[1] == len(time):\n",
    "    # signals: (channel, time)\n",
    "    ds = xr.Dataset(\n",
    "        {\"signals\": ((\"channel\", \"time\"), signals)},\n",
    "        coords={\"time\": (\"time\", time), \"channel\": (\"channel\", np.arange(signals.shape[0]))},\n",
    "    )\n",
    "elif signals.shape[0] == len(time):\n",
    "    # signals: (time, channel)\n",
    "    ds = xr.Dataset(\n",
    "        {\"signals\": ((\"time\", \"channel\"), signals)},\n",
    "        coords={\"time\": (\"time\", time), \"channel\": (\"channel\", np.arange(signals.shape[1]))},\n",
    "    )\n",
    "else:\n",
    "    # Unknown layout; wrap generically\n",
    "    ds = xr.Dataset({\"signals\": ((\"dim0\", \"dim1\"), signals)})\n",
    "\n",
    "print(\"\\nConstructed xarray Dataset:\")\n",
    "print(ds)\n",
    "\n",
    "# Example lazy access: slice without pulling everything\n",
    "if \"time\" in ds.dims:\n",
    "    print(\"\\nExample slice:\", ds[\"signals\"].isel(time=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53cd1323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x31b3ad6c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "import zarr\n",
    "\n",
    "os.environ.setdefault(\"AWS_DEFAULT_REGION\", \"eu-north-1\")\n",
    "fs = s3fs.S3FileSystem(anon=False, client_kwargs={\"region_name\": \"eu-north-1\"})\n",
    "\n",
    "# Load raw SPARC-FUSE Zarr (as you did)\n",
    "raw_store = zarr.storage.FSStore(\"s3://sparc-fuse-demo-ab-2025/20_1021_std.zarr\", fs=fs)\n",
    "try:\n",
    "    root = zarr.open_consolidated(raw_store)\n",
    "except Exception:\n",
    "    root = zarr.open(raw_store)\n",
    "\n",
    "signals = root[\"signals\"][:]\n",
    "time = root[\"time\"][:]\n",
    "\n",
    "# Build the Xarray Dataset\n",
    "ds = xr.Dataset(\n",
    "    {\"signals\": ((\"channel\", \"time\"), signals)},\n",
    "    coords={\"time\": (\"time\", time), \"channel\": (\"channel\", np.arange(signals.shape[0]))},\n",
    ")\n",
    "\n",
    "# Write out a self-describing Zarr store with consolidated metadata\n",
    "out_path = \"s3://sparc-fuse-demo-ab-2025/20_1021_std_xarray.zarr\"\n",
    "out_store = zarr.storage.FSStore(out_path, fs=fs)\n",
    "ds.to_zarr(out_store, mode=\"w\", consolidated=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f765c204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 56MB\n",
      "Dimensions:  (channel: 4, time: 1408754)\n",
      "Coordinates:\n",
      "  * channel  (channel) int64 32B 0 1 2 3\n",
      "  * time     (time) float64 11MB 0.0 0.01 0.02 ... 1.409e+04 1.409e+04 1.409e+04\n",
      "Data variables:\n",
      "    signals  (channel, time) float64 45MB dask.array<chunksize=(1, 88048), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "ds = xr.open_zarr(\"s3://sparc-fuse-demo-ab-2025/20_1021_std_xarray.zarr\", consolidated=True)\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43c9762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting primary file primary/sub-ChAT-Male-Subject-1/20_1021.acq to Zarr...\n",
      "[WARN] Failed to load descriptor from ./mapping_schemes/mapping_scheme_400_adi.py: No module named 'adi._adi_cffi'\n",
      "[WARN] Failed to load descriptor from ./mapping_schemes/mapping_scheme_378.py: No module named 'adi._adi_cffi'\n",
      "[INFO] Loaded 32 descriptor(s) from ./mapping_schemes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset 224: 100%|██████████| 1/1 [00:07<00:00,  7.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading raw Zarr to s3://sparc-fuse-demo-ab-2025/20_1021_std.zarr ...\n",
      "Consolidating raw Zarr metadata...\n",
      "Constructed xarray.Dataset: <xarray.Dataset> Size: 56MB\n",
      "Dimensions:  (channel: 4, time: 1408754)\n",
      "Coordinates:\n",
      "  * time     (time) float64 11MB 0.0 0.01 0.02 ... 1.409e+04 1.409e+04 1.409e+04\n",
      "  * channel  (channel) int64 32B 0 1 2 3\n",
      "Data variables:\n",
      "    signals  (channel, time) float64 45MB -0.2869 -0.3036 ... 0.0008336\n",
      "Writing self-describing Zarr to s3://sparc-fuse-demo-ab-2025/20_1021_std_xarray.zarr ...\n",
      "upload: ./latest.json to s3://sparc-fuse-demo-ab-2025/latest.json   \n",
      "\n",
      "✅ Preparation complete.\n",
      "Self-describing Zarr at: s3://sparc-fuse-demo-ab-2025/20_1021_std_xarray.zarr\n",
      "Manifest at: s3://sparc-fuse-demo-ab-2025/latest.json\n"
     ]
    }
   ],
   "source": [
    "# === Cloud-first preparation: convert a SPARC file, push to S3, consolidate, and emit an Xarray-compatible Zarr ===\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "import zarr\n",
    "from sparc_fuse_core import list_primary_files, download_and_convert_sparc_data\n",
    "\n",
    "# ---------- parameters (demo) ----------\n",
    "DATASET_ID = 224\n",
    "BUCKET = \"sparc-fuse-demo-ab-2025\"        # your bucket name\n",
    "REGION = \"eu-north-1\"                     # bucket region\n",
    "RAW_ZARR = \"20_1021_std.zarr\"\n",
    "XARRAY_ZARR = \"20_1021_std_xarray.zarr\"\n",
    "\n",
    "# Ensure AWS region is visible to libraries\n",
    "os.environ.setdefault(\"AWS_DEFAULT_REGION\", REGION)\n",
    "\n",
    "# 1. Convert a single primary file to Zarr locally\n",
    "files, _ = list_primary_files(DATASET_ID)\n",
    "primary_path = files[0][\"path\"].replace(\"files/\", \"\")\n",
    "print(f\"Converting primary file {primary_path} to Zarr...\")\n",
    "download_and_convert_sparc_data(\n",
    "    DATASET_ID,\n",
    "    primary_paths=primary_path,\n",
    "    output_dir=\"./output_single\",\n",
    "    file_format=\"zarr\"\n",
    ")\n",
    "\n",
    "# 2. Sync raw Zarr to S3\n",
    "print(f\"Uploading raw Zarr to s3://{BUCKET}/{RAW_ZARR} ...\")\n",
    "subprocess.run([\n",
    "    \"aws\", \"s3\", \"sync\",\n",
    "    f\"./output_single/{RAW_ZARR}\",\n",
    "    f\"s3://{BUCKET}/{RAW_ZARR}\",\n",
    "    \"--region\", REGION\n",
    "], check=True)\n",
    "\n",
    "# 3. Consolidate metadata on S3 (raw store)\n",
    "fs = s3fs.S3FileSystem(anon=False, client_kwargs={\"region_name\": REGION})\n",
    "raw_store = zarr.storage.FSStore(f\"s3://{BUCKET}/{RAW_ZARR}\", fs=fs)\n",
    "print(\"Consolidating raw Zarr metadata...\")\n",
    "zarr.consolidate_metadata(raw_store)\n",
    "\n",
    "# 4. Load raw Zarr and assemble an xarray.Dataset\n",
    "try:\n",
    "    root = zarr.open_consolidated(raw_store)\n",
    "except Exception:\n",
    "    root = zarr.open(raw_store)\n",
    "\n",
    "signals = root[\"signals\"][:]  # (channel, time)\n",
    "time = root[\"time\"][:]\n",
    "ds = xr.Dataset(\n",
    "    {\"signals\": ((\"channel\", \"time\"), signals)},\n",
    "    coords={\"time\": (\"time\", time), \"channel\": (\"channel\", np.arange(signals.shape[0]))},\n",
    ")\n",
    "\n",
    "print(\"Constructed xarray.Dataset:\", ds)\n",
    "\n",
    "# 5. Write out self-describing (Xarray-compatible) Zarr to S3\n",
    "xarray_store = zarr.storage.FSStore(f\"s3://{BUCKET}/{XARRAY_ZARR}\", fs=fs)\n",
    "print(f\"Writing self-describing Zarr to s3://{BUCKET}/{XARRAY_ZARR} ...\")\n",
    "ds.to_zarr(xarray_store, mode=\"w\", consolidated=True)\n",
    "\n",
    "# 6. Emit a lightweight discovery manifest\n",
    "manifest = {\n",
    "    \"dataset_id\": DATASET_ID,\n",
    "    \"zarr_path\": f\"s3://{BUCKET}/{XARRAY_ZARR}\",\n",
    "    \"generated_at\": f\"{__import__('datetime').datetime.utcnow().isoformat()}Z\",\n",
    "    \"file_format\": \"zarr\"\n",
    "}\n",
    "with open(\"latest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "subprocess.run([\n",
    "    \"aws\", \"s3\", \"cp\", \"latest.json\",\n",
    "    f\"s3://{BUCKET}/latest.json\",\n",
    "    \"--region\", REGION\n",
    "], check=True)\n",
    "\n",
    "print(\"\\n✅ Preparation complete.\")\n",
    "print(\"Self-describing Zarr at:\", f\"s3://{BUCKET}/{XARRAY_ZARR}\")\n",
    "print(\"Manifest at:\", f\"s3://{BUCKET}/latest.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf98f5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure (metadata only):\n",
      "<xarray.Dataset> Size: 56MB\n",
      "Dimensions:  (channel: 4, time: 1408754)\n",
      "Coordinates:\n",
      "  * channel  (channel) int64 32B 0 1 2 3\n",
      "  * time     (time) float64 11MB 0.0 0.01 0.02 ... 1.409e+04 1.409e+04 1.409e+04\n",
      "Data variables:\n",
      "    signals  (channel, time) float64 45MB dask.array<chunksize=(1, 88048), meta=np.ndarray>\n",
      "\n",
      "Example lazy slice (first 1k timepoints of channel 0):\n",
      "<xarray.DataArray 'signals' (time: 1000)> Size: 8kB\n",
      "dask.array<getitem, shape=(1000,), dtype=float64, chunksize=(1000,), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "    channel  int64 8B 0\n",
      "  * time     (time) float64 8kB 0.0 0.01 0.02 0.03 0.04 ... 9.96 9.97 9.98 9.99\n"
     ]
    }
   ],
   "source": [
    "# === Consume the prepared self-describing Zarr lazily ===\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "# Parameters (must match what was produced)\n",
    "ZARR_PATH = \"s3://sparc-fuse-demo-ab-2025/20_1021_std_xarray.zarr\"\n",
    "os.environ.setdefault(\"AWS_DEFAULT_REGION\", \"eu-north-1\")\n",
    "\n",
    "# Lazy open\n",
    "ds_lazy = xr.open_zarr(ZARR_PATH, consolidated=True)\n",
    "print(\"Dataset structure (metadata only):\")\n",
    "print(ds_lazy)\n",
    "\n",
    "# Example zero-copy slice (only fetches needed chunk)\n",
    "print(\"\\nExample lazy slice (first 1k timepoints of channel 0):\")\n",
    "print(ds_lazy[\"signals\"].isel(channel=0, time=slice(0, 1000)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a40628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Failed to load descriptor from ./mapping_schemes/mapping_scheme_400_adi.py: No module named 'adi._adi_cffi'\n",
      "[WARN] Failed to load descriptor from ./mapping_schemes/mapping_scheme_378.py: No module named 'adi._adi_cffi'\n",
      "[INFO] Loaded 32 descriptor(s) from ./mapping_schemes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset 224: 100%|██████████| 1/1 [00:07<00:00,  7.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./latest.json to s3://sparc-fuse-demo-ab-2025/latest.json   \n",
      "✅ Preparation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "import zarr\n",
    "from sparc_fuse_core import list_primary_files, download_and_convert_sparc_data\n",
    "\n",
    "# Parameters\n",
    "DATASET_ID = 224\n",
    "BUCKET = \"sparc-fuse-demo-ab-2025\"\n",
    "REGION = \"eu-north-1\"\n",
    "RAW_ZARR = \"20_1021_std.zarr\"\n",
    "XARRAY_ZARR = \"20_1021_std_xarray.zarr\"\n",
    "\n",
    "# AWS region setup\n",
    "os.environ.setdefault(\"AWS_DEFAULT_REGION\", REGION)\n",
    "\n",
    "# Convert SPARC file to Zarr locally\n",
    "files, _ = list_primary_files(DATASET_ID)\n",
    "primary_path = files[0][\"path\"].replace(\"files/\", \"\")\n",
    "download_and_convert_sparc_data(\n",
    "    DATASET_ID,\n",
    "    primary_paths=primary_path,\n",
    "    output_dir=\"./output_single\",\n",
    "    file_format=\"zarr\"\n",
    ")\n",
    "\n",
    "# Upload raw Zarr to S3\n",
    "subprocess.run([\n",
    "    \"aws\", \"s3\", \"sync\",\n",
    "    f\"./output_single/{RAW_ZARR}\",\n",
    "    f\"s3://{BUCKET}/{RAW_ZARR}\",\n",
    "    \"--region\", REGION\n",
    "], check=True)\n",
    "\n",
    "# Consolidate metadata\n",
    "fs = s3fs.S3FileSystem(anon=False, client_kwargs={\"region_name\": REGION})\n",
    "raw_store = zarr.storage.FSStore(f\"s3://{BUCKET}/{RAW_ZARR}\", fs=fs)\n",
    "zarr.consolidate_metadata(raw_store)\n",
    "\n",
    "# Create Xarray-compatible Zarr\n",
    "root = zarr.open_consolidated(raw_store)\n",
    "signals = root[\"signals\"][:]\n",
    "time = root[\"time\"][:]\n",
    "ds = xr.Dataset(\n",
    "    {\"signals\": ((\"channel\", \"time\"), signals)},\n",
    "    coords={\"time\": (\"time\", time), \"channel\": (\"channel\", np.arange(signals.shape[0]))},\n",
    ")\n",
    "\n",
    "# Write Xarray-compatible Zarr to S3\n",
    "xarray_store = zarr.storage.FSStore(f\"s3://{BUCKET}/{XARRAY_ZARR}\", fs=fs)\n",
    "ds.to_zarr(xarray_store, mode=\"w\", consolidated=True)\n",
    "\n",
    "# Generate discovery manifest\n",
    "manifest = {\n",
    "    \"dataset_id\": DATASET_ID,\n",
    "    \"zarr_path\": f\"s3://{BUCKET}/{XARRAY_ZARR}\",\n",
    "    \"generated_at\": f\"{__import__('datetime').datetime.utcnow().isoformat()}Z\",\n",
    "    \"file_format\": \"zarr\"\n",
    "}\n",
    "with open(\"latest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "subprocess.run([\n",
    "    \"aws\", \"s3\", \"cp\", \"latest.json\",\n",
    "    f\"s3://{BUCKET}/latest.json\",\n",
    "    \"--region\", REGION\n",
    "], check=True)\n",
    "\n",
    "print(\"✅ Preparation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59a3c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 56MB\n",
      "Dimensions:  (channel: 4, time: 1408754)\n",
      "Coordinates:\n",
      "  * channel  (channel) int64 32B 0 1 2 3\n",
      "  * time     (time) float64 11MB 0.0 0.01 0.02 ... 1.409e+04 1.409e+04 1.409e+04\n",
      "Data variables:\n",
      "    signals  (channel, time) float64 45MB dask.array<chunksize=(1, 88048), meta=np.ndarray>\n",
      "<xarray.DataArray 'signals' (time: 1000)> Size: 8kB\n",
      "dask.array<getitem, shape=(1000,), dtype=float64, chunksize=(1000,), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "    channel  int64 8B 0\n",
      "  * time     (time) float64 8kB 0.0 0.01 0.02 0.03 0.04 ... 9.96 9.97 9.98 9.99\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "os.environ.setdefault(\"AWS_DEFAULT_REGION\", \"eu-north-1\")\n",
    "\n",
    "# Lazily open the data directly from S3\n",
    "ds = xr.open_zarr(\n",
    "    \"s3://sparc-fuse-demo-ab-2025/20_1021_std_xarray.zarr\",\n",
    "    consolidated=True\n",
    ")\n",
    "print(ds)\n",
    "\n",
    "# Stream just a slice you need\n",
    "subset = ds[\"signals\"].sel(channel=0).isel(time=slice(0, 1000))\n",
    "print(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "505d163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Failed to load descriptor from ./mapping_schemes/mapping_scheme_400_adi.py: No module named 'adi._adi_cffi'\n",
      "[WARN] Failed to load descriptor from ./mapping_schemes/mapping_scheme_378.py: No module named 'adi._adi_cffi'\n",
      "[INFO] Loaded 32 descriptor(s) from ./mapping_schemes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset 224: 100%|██████████| 1/1 [00:08<00:00,  8.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./latest.json to s3://sparc-fuse-demo-ab-2025/latest.json   \n",
      "✅ Preparation complete.\n"
     ]
    }
   ],
   "source": [
    "from sparc_fuse_core import (\n",
    "    list_primary_files, download_and_convert_sparc_data,\n",
    "    upload_to_s3, consolidate_s3_metadata,\n",
    "    create_xarray_zarr_from_raw, generate_and_upload_manifest\n",
    ")\n",
    "\n",
    "# Parameters\n",
    "DATASET_ID = 224\n",
    "BUCKET = \"sparc-fuse-demo-ab-2025\"\n",
    "REGION = \"eu-north-1\"\n",
    "RAW_ZARR = \"20_1021_std.zarr\"\n",
    "XARRAY_ZARR = \"20_1021_std_xarray.zarr\"\n",
    "\n",
    "# Convert SPARC file to Zarr locally\n",
    "files, _ = list_primary_files(DATASET_ID)\n",
    "primary_path = files[0][\"path\"].replace(\"files/\", \"\")\n",
    "download_and_convert_sparc_data(\n",
    "    DATASET_ID,\n",
    "    primary_paths=primary_path,\n",
    "    output_dir=\"./output_single\",\n",
    "    file_format=\"zarr\"\n",
    ")\n",
    "\n",
    "# Upload raw Zarr to S3\n",
    "upload_to_s3(f\"./output_single/{RAW_ZARR}\", BUCKET, RAW_ZARR, REGION)\n",
    "\n",
    "# Consolidate metadata\n",
    "consolidate_s3_metadata(BUCKET, RAW_ZARR, REGION)\n",
    "\n",
    "# Create Xarray-compatible Zarr and upload to S3\n",
    "create_xarray_zarr_from_raw(BUCKET, RAW_ZARR, XARRAY_ZARR, REGION)\n",
    "\n",
    "# Generate discovery manifest and upload\n",
    "generate_and_upload_manifest(DATASET_ID, BUCKET, XARRAY_ZARR, REGION)\n",
    "\n",
    "print(\"✅ Preparation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3e1d31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 56MB\n",
      "Dimensions:  (channel: 4, time: 1408754)\n",
      "Coordinates:\n",
      "  * channel  (channel) int64 32B 0 1 2 3\n",
      "  * time     (time) float64 11MB 0.0 0.01 0.02 ... 1.409e+04 1.409e+04 1.409e+04\n",
      "Data variables:\n",
      "    signals  (channel, time) float64 45MB dask.array<chunksize=(1, 88048), meta=np.ndarray>\n",
      "<xarray.DataArray 'signals' (time: 1000)> Size: 8kB\n",
      "dask.array<getitem, shape=(1000,), dtype=float64, chunksize=(1000,), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "    channel  int64 8B 0\n",
      "  * time     (time) float64 8kB 0.0 0.01 0.02 0.03 0.04 ... 9.96 9.97 9.98 9.99\n"
     ]
    }
   ],
   "source": [
    "from sparc_fuse_core import open_zarr_from_s3\n",
    "\n",
    "# Open dataset lazily from S3\n",
    "ds = open_zarr_from_s3(\n",
    "    bucket=\"sparc-fuse-demo-ab-2025\",\n",
    "    zarr_path=\"20_1021_std_xarray.zarr\"\n",
    ")\n",
    "\n",
    "print(ds)  # Immediately available metadata, lazy data loading\n",
    "\n",
    "# Stream a specific data slice\n",
    "subset = ds[\"signals\"].sel(channel=0).isel(time=slice(0, 1000))\n",
    "print(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e679c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparc_vns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
